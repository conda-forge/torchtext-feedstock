From ab457e9b14786f88f0d1054c2fd13350408f27ba Mon Sep 17 00:00:00 2001
From: "H. Vetinari" <h.vetinari@gmx.com>
Date: Wed, 26 Mar 2025 10:50:35 +1100
Subject: [PATCH 6/6] allow torch.load to load more than weights, as previously

---
 test/integration_tests/test_roberta_models.py        |  2 +-
 test/integration_tests/test_t5_models.py             |  2 +-
 test/torchtext_unittest/prototype/test_functional.py |  4 ++--
 test/torchtext_unittest/prototype/test_transforms.py |  4 ++--
 test/torchtext_unittest/prototype/test_vectors.py    |  4 ++--
 test/torchtext_unittest/test_transforms.py           | 12 ++++++------
 test/torchtext_unittest/test_vocab.py                |  4 ++--
 7 files changed, 16 insertions(+), 16 deletions(-)

diff --git a/test/integration_tests/test_roberta_models.py b/test/integration_tests/test_roberta_models.py
index 9c1e53f8b..7d12ee586 100644
--- a/test/integration_tests/test_roberta_models.py
+++ b/test/integration_tests/test_roberta_models.py
@@ -47,7 +47,7 @@ class TestRobertaEncoders(TorchtextTestCase):
 
         model_input = torch.tensor(transform([test_text]))
         actual = model(model_input)
-        expected = torch.load(expected_asset_path)
+        expected = torch.load(expected_asset_path, weights_only=False)
         torch.testing.assert_close(actual, expected)
 
     @parameterized.expand(["jit", "not_jit"])
diff --git a/test/integration_tests/test_t5_models.py b/test/integration_tests/test_t5_models.py
index 0bacca430..ba1b4bab5 100644
--- a/test/integration_tests/test_t5_models.py
+++ b/test/integration_tests/test_t5_models.py
@@ -80,7 +80,7 @@ class TestT5Model(TorchtextTestCase):
         else:
             actual = model(encoder_tokens=model_input)["decoder_output"]
 
-        expected = torch.load(expected_asset_path)
+        expected = torch.load(expected_asset_path, weights_only=False)
         torch.testing.assert_close(actual, expected, atol=1e-04, rtol=2.5e-06)
 
     def _t5_get_encoder(self, model, model_input, encoder_output):
diff --git a/test/torchtext_unittest/prototype/test_functional.py b/test/torchtext_unittest/prototype/test_functional.py
index f6da11e80..888c2aeef 100644
--- a/test/torchtext_unittest/prototype/test_functional.py
+++ b/test/torchtext_unittest/prototype/test_functional.py
@@ -84,7 +84,7 @@ class TestFunctional(TorchtextTestCase):
             save_path = os.path.join(self.test_dir, "ben_pybind.pt")
             ben = basic_english_normalize()
             torch.save(ben, save_path)
-            loaded_ben = torch.load(save_path)
+            loaded_ben = torch.load(save_path, weights_only=False)
             self.assertEqual(loaded_ben(test_sample), ref_results)
 
         with self.subTest("torchscript"):
@@ -93,5 +93,5 @@ class TestFunctional(TorchtextTestCase):
             # Not expect users to use the torchbind version on eager mode but still need a CI test here.
             ben = basic_english_normalize().__prepare_scriptable__()
             torch.save(ben, save_path)
-            loaded_ben = torch.load(save_path)
+            loaded_ben = torch.load(save_path, weights_only=False)
             self.assertEqual(loaded_ben(test_sample), ref_results)
diff --git a/test/torchtext_unittest/prototype/test_transforms.py b/test/torchtext_unittest/prototype/test_transforms.py
index 3b28b0786..aecfc05c4 100644
--- a/test/torchtext_unittest/prototype/test_transforms.py
+++ b/test/torchtext_unittest/prototype/test_transforms.py
@@ -125,7 +125,7 @@ class TestTransforms(TorchtextTestCase):
             save_path = os.path.join(self.test_dir, "spm_pybind.pt")
             spm = sentencepiece_tokenizer((model_path))
             torch.save(spm, save_path)
-            loaded_spm = torch.load(save_path)
+            loaded_spm = torch.load(save_path, weights_only=False)
             self.assertEqual(expected, loaded_spm(input))
 
         with self.subTest("torchscript"):
@@ -134,5 +134,5 @@ class TestTransforms(TorchtextTestCase):
             # Not expect users to use the torchbind version on eager mode but still need a CI test here.
             spm = sentencepiece_tokenizer((model_path)).__prepare_scriptable__()
             torch.save(spm, save_path)
-            loaded_spm = torch.load(save_path)
+            loaded_spm = torch.load(save_path, weights_only=False)
             self.assertEqual(expected, loaded_spm(input))
diff --git a/test/torchtext_unittest/prototype/test_vectors.py b/test/torchtext_unittest/prototype/test_vectors.py
index 2c001cc26..aa2a3f8a3 100644
--- a/test/torchtext_unittest/prototype/test_vectors.py
+++ b/test/torchtext_unittest/prototype/test_vectors.py
@@ -141,7 +141,7 @@ class TestVectors(TorchtextTestCase):
         with self.subTest("pybind"):
             vector_path = os.path.join(self.test_dir, "vectors_pybind.pt")
             torch.save(vectors_obj, vector_path)
-            loaded_vectors_obj = torch.load(vector_path)
+            loaded_vectors_obj = torch.load(vector_path, weights_only=False)
 
             self.assertEqual(loaded_vectors_obj["a"], tensorA)
             self.assertEqual(loaded_vectors_obj["b"], tensorB)
@@ -152,7 +152,7 @@ class TestVectors(TorchtextTestCase):
             # Call the __prepare_scriptable__() func and convert the building block to the torbhind version
             # Not expect users to use the torchbind version on eager mode but still need a CI test here.
             torch.save(vectors_obj.__prepare_scriptable__(), vector_path)
-            loaded_vectors_obj = torch.load(vector_path)
+            loaded_vectors_obj = torch.load(vector_path, weights_only=False)
 
             self.assertEqual(loaded_vectors_obj["a"], tensorA)
             self.assertEqual(loaded_vectors_obj["b"], tensorB)
diff --git a/test/torchtext_unittest/test_transforms.py b/test/torchtext_unittest/test_transforms.py
index 296970ea0..3266ce457 100644
--- a/test/torchtext_unittest/test_transforms.py
+++ b/test/torchtext_unittest/test_transforms.py
@@ -732,7 +732,7 @@ class TestGPT2BPETokenizer(TorchtextTestCase):
         tokenizer = self._load_tokenizer(test_scripting=False, return_tokens=False)
         tokenizer_path = os.path.join(self.test_dir, "gpt2_tokenizer_pybind.pt")
         torch.save(tokenizer, tokenizer_path)
-        loaded_tokenizer = torch.load(tokenizer_path)
+        loaded_tokenizer = torch.load(tokenizer_path, weights_only=False)
         self._gpt2_bpe_tokenizer((loaded_tokenizer))
 
     def test_gpt2_bpe_tokenizer_save_load_torchscript(self) -> None:
@@ -741,7 +741,7 @@ class TestGPT2BPETokenizer(TorchtextTestCase):
         # Call the __prepare_scriptable__() func and convert the building block to the torbhind version
         # Not expect users to use the torchbind version on eager mode but still need a CI test here.
         torch.save(tokenizer.__prepare_scriptable__(), tokenizer_path)
-        loaded_tokenizer = torch.load(tokenizer_path)
+        loaded_tokenizer = torch.load(tokenizer_path, weights_only=False)
         self._gpt2_bpe_tokenizer((loaded_tokenizer))
 
 
@@ -994,7 +994,7 @@ class TestCLIPTokenizer(TorchtextTestCase):
         tokenizer = self._load_tokenizer(init_using_merge_only=True, test_scripting=False, return_tokens=False)
         tokenizer_path = os.path.join(self.test_dir, "gpt2_tokenizer_pybind.pt")
         torch.save(tokenizer, tokenizer_path)
-        loaded_tokenizer = torch.load(tokenizer_path)
+        loaded_tokenizer = torch.load(tokenizer_path, weights_only=False)
         self._clip_tokenizer((loaded_tokenizer))
 
     def test_clip_tokenizer_save_load_torchscript(self) -> None:
@@ -1003,7 +1003,7 @@ class TestCLIPTokenizer(TorchtextTestCase):
         # Call the __prepare_scriptable__() func and convert the building block to the torbhind version
         # Not expect users to use the torchbind version on eager mode but still need a CI test here.
         torch.save(tokenizer.__prepare_scriptable__(), tokenizer_path)
-        loaded_tokenizer = torch.load(tokenizer_path)
+        loaded_tokenizer = torch.load(tokenizer_path, weights_only=False)
         self._clip_tokenizer((loaded_tokenizer))
 
 
@@ -1172,7 +1172,7 @@ class TestBERTTokenizer(TorchtextTestCase):
             self._bert_tokenizer((loaded_tokenizer), do_lower_case=do_lower_case)
         else:
             torch.save(tokenizer, tokenizer_path)
-            loaded_tokenizer = torch.load(tokenizer_path)
+            loaded_tokenizer = torch.load(tokenizer_path, weights_only=False)
             self._bert_tokenizer((loaded_tokenizer), do_lower_case=do_lower_case)
 
 
@@ -1239,7 +1239,7 @@ class TestRegexTokenizer(TorchtextTestCase):
 
             tokenizer = RegexTokenizer(self.patterns_list)
             torch.save(tokenizer, save_path)
-            loaded_tokenizer = torch.load(save_path)
+            loaded_tokenizer = torch.load(save_path, weights_only=False)
             results = loaded_tokenizer(self.test_sample)
             self.assertEqual(results, self.ref_results)
 
diff --git a/test/torchtext_unittest/test_vocab.py b/test/torchtext_unittest/test_vocab.py
index ca310e1a6..bca28a7c8 100644
--- a/test/torchtext_unittest/test_vocab.py
+++ b/test/torchtext_unittest/test_vocab.py
@@ -206,7 +206,7 @@ class TestVocab(TorchtextTestCase):
         with self.subTest("pybind"):
             vocab_path = os.path.join(self.test_dir, "vocab_pybind.pt")
             torch.save(v, vocab_path)
-            loaded_v = torch.load(vocab_path)
+            loaded_v = torch.load(vocab_path, weigths_only=False)
             self.assertEqual(v.get_itos(), expected_itos)
             self.assertEqual(dict(loaded_v.get_stoi()), expected_stoi)
             self.assertEqual(v["not in vocab"], 0)
@@ -216,7 +216,7 @@ class TestVocab(TorchtextTestCase):
             # Call the __prepare_scriptable__() func and convert the building block to the torbhind version
             # Not expect users to use the torchbind version on eager mode but still need a CI test here.
             torch.save(v.__prepare_scriptable__(), vocab_path)
-            loaded_v = torch.load(vocab_path)
+            loaded_v = torch.load(vocab_path, weigths_only=False)
             self.assertEqual(v.get_itos(), expected_itos)
             self.assertEqual(dict(loaded_v.get_stoi()), expected_stoi)
             self.assertEqual(v["not in vocab"], 0)
